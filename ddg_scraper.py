import subprocess
import streamlit as st

import asyncio
import sys

# ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Windows ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ ProactorEventLoopPolicy ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö subprocess
if sys.platform.startswith("win"):
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

import streamlit as st
from urllib.parse import quote_plus
# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
# Ensure Playwright has downloaded its browser binaries
# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
@st.cache_resource
def _install_playwright_browsers():
    """
    On Streamlit Cloud the playwright CLI may not be on PATH,
    so we invoke it via Python: `python -m playwright install`.
    """
    cmd = [
        sys.executable, "-m", "playwright", "install", "--with-deps"
    ]
    # Let the logs show up in the Cloud logs so we can debug if it still fails
    subprocess.run(cmd, check=True)

_install_playwright_browsers()

# now it‚Äôs safe to import Playwright
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
import time
import pandas as pd
import io
import datetime

# main.py (top of file)

# --- Scraper function using Playwright ---
def scrape_duckduckgo(query: str,
                       max_clicks: int = 20,
                       delay: float = 1.0) -> pd.DataFrame:
    """
    Scrape DuckDuckGo search results dynamically via Playwright.

    Args:
        query (str): Search query string.
        max_clicks (int): Maximum times to click "‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°" button.
        delay (float): Seconds to wait after each click.

    Returns:
        pd.DataFrame: DataFrame with columns [title, url, scraped_at].
    """
    url = f"https://duckduckgo.com/?q={quote_plus(query)}&t=h_"
    results = []
    scrape_time = datetime.datetime.now().isoformat()

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        page.goto(url)
        page.wait_for_selector("article[data-testid='result']", timeout=10000)

        for _ in range(max_clicks):
            # Scroll to bottom so button is in view
            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            try:
                page.wait_for_selector("button#more-results", timeout=5000)
            except:
                break

            page.click("button#more-results")
            page.wait_for_load_state("networkidle")
            time.sleep(delay)

        html = page.content()
        browser.close()

    # Parse HTML with BeautifulSoup
    soup = BeautifulSoup(html, "html.parser")
    for article in soup.select("article[data-testid='result']"):
        a = article.select_one("a[data-testid='result-title-a']")
        if not a:
            continue
        title = a.get_text(strip=True)
        href = a.get('href')
        results.append({
            "title": title,
            "url": href,
            "scraped_at": scrape_time
        })

    return pd.DataFrame(results)

# --- Streamlit App ---
def main():
    st.set_page_config(page_title="ü¶Ü DuckDuckGo Scraper", layout="wide")
    st.title("ü¶Ü DuckDuckGo Playwright Scraper")
    st.markdown("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏Å‡∏£‡∏≠‡∏Å‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≤‡∏á ‡πÜ ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏Ñ‡∏•‡∏¥‡∏Å Search ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå")

    st.sidebar.header("‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Scraper")
    max_clicks = st.sidebar.slider("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏•‡∏¥‡∏Å '‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°' ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î", 1, 50, 20)
    delay = st.sidebar.slider("‡∏´‡∏ô‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏´‡∏•‡∏±‡∏á‡∏Ñ‡∏•‡∏¥‡∏Å (‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)", 0.5, 5.0, 1.0)

    query = st.text_input("üîç ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤:", value="", placeholder="‡πÄ‡∏ä‡πà‡∏ô artificial intelligence")
    if st.button("Search"):
        if not query.strip():
            st.error("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏Ñ‡∏•‡∏¥‡∏Å Search")
            return

        with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤... ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏≠‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà"):
            try:
                df = scrape_duckduckgo(query, max_clicks=max_clicks, delay=delay)
            except Exception as e:
                st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡πÅ‡∏Ñ‡∏£‡∏õ: {e}")
                return

        if df.empty:
            st.warning("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÉ‡∏î ‡πÜ")
        else:
            st.success(f"‡∏û‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå {len(df)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
            st.dataframe(df, use_container_width=True)

            # Download buttons
            csv = df.to_csv(index=False, encoding='utf-8-sig')
            st.download_button("‚¨áÔ∏è ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î CSV", data=csv, file_name=f"ddg_{int(time.time())}.csv")

            excel_buffer = io.BytesIO()
            with pd.ExcelWriter(excel_buffer, engine='xlsxwriter') as writer:
                df.to_excel(writer, index=False, sheet_name='Results')
            st.download_button("‚¨áÔ∏è ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Excel", data=excel_buffer.getvalue(), file_name=f"ddg_{int(time.time())}.xlsx")

if __name__ == "__main__":
    main()