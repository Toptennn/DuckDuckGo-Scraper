import time

import streamlit as st
import asyncio
import sys
if sys.platform.startswith("win"):
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())


from scraper import DuckDuckGoScraper,create_download_files, display_error_suggestions, display_no_results_info

def main():
    """Main Streamlit application."""
    st.set_page_config(page_title="ü¶Ü DuckDuckGo Scraper", layout="wide")
    st.title("ü¶Ü DuckDuckGo Playwright Scraper")
    st.markdown("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏Å‡∏£‡∏≠‡∏Å‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≤‡∏á ‡πÜ ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏Ñ‡∏•‡∏¥‡∏Å Search ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå")

    # Sidebar configuration
    st.sidebar.header("‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Scraper")
    max_pages = st.sidebar.slider("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏ô‡πâ‡∏≤", 1, 50, 20)

    # Main input
    query = st.text_input(
        "üîç ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤:", 
        value="", 
        placeholder="‡πÄ‡∏ä‡πà‡∏ô artificial intelligence"
    )
    
    if st.button("Search"):
        if not query.strip():
            st.error("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏Ñ‡∏•‡∏¥‡∏Å Search")
            return

        scraper = DuckDuckGoScraper()
        
        with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤... ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏≠‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà"):
            try:
                df, pages_retrieved = scraper.scrape(query, max_pages, headless=True)
            except Exception as e:
                st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡πÅ‡∏Ñ‡∏£‡∏õ: {e}")
                display_error_suggestions()
                return

        # Display results
        if df.empty:
            st.warning("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÉ‡∏î ‡πÜ")
            display_no_results_info()
        else:
            st.success(f"‡∏û‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå {len(df)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ ‡∏à‡∏≤‡∏Å {pages_retrieved} ‡∏´‡∏ô‡πâ‡∏≤")
            
            if pages_retrieved < max_pages:
                st.info(f"‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: Scraping ‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡πÄ‡∏û‡∏µ‡∏¢‡∏á {pages_retrieved} ‡∏´‡∏ô‡πâ‡∏≤ ‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ {max_pages} ‡∏´‡∏ô‡πâ‡∏≤")
            
            st.dataframe(df, use_container_width=True)

            # Download buttons
            csv_data, excel_data = create_download_files(df)
            timestamp = int(time.time())
            
            st.download_button(
                "‚¨áÔ∏è ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î CSV", 
                data=csv_data, 
                file_name=f"ddg_{timestamp}.csv"
            )
            
            st.download_button(
                "‚¨áÔ∏è ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Excel", 
                data=excel_data, 
                file_name=f"ddg_{timestamp}.xlsx"
            )


if __name__ == "__main__":
    main()
