import time
import streamlit as st
import asyncio
import sys

if sys.platform.startswith("win"):
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

from scraper import DuckDuckGoScraper, create_download_files, display_error_suggestions, display_no_results_info

def _add_normal_query(queries, query_parts):
    if queries.get('normal_query'):
        query_parts.append(queries['normal_query'])

def _add_exact_phrase(queries, query_parts):
    if queries.get('exact_phrase'):
        query_parts.append(f'"{queries["exact_phrase"]}"')

def _add_semantic_query(queries, query_parts):
    if queries.get('semantic_query'):
        query_parts.append(f'~"{queries["semantic_query"]}"')

def _add_include_terms(queries, query_parts):
    if queries.get('include_terms'):
        terms = [term.strip() for term in queries['include_terms'].split(',') if term.strip()]
        query_parts.extend([f"+{term}" for term in terms])

def _add_exclude_terms(queries, query_parts):
    if queries.get('exclude_terms'):
        terms = [term.strip() for term in queries['exclude_terms'].split(',') if term.strip()]
        query_parts.extend([f"-{term}" for term in terms])

def _add_filetype(queries, query_parts):
    if queries.get('filetype'):
        query_parts.append(f"filetype:{queries['filetype']}")

def _add_site_include(queries, query_parts):
    if queries.get('site_include'):
        query_parts.append(f"site:{queries['site_include']}")

def _add_site_exclude(queries, query_parts):
    if queries.get('site_exclude'):
        query_parts.append(f"-site:{queries['site_exclude']}")

def _add_intitle(queries, query_parts):
    if queries.get('intitle'):
        query_parts.append(f"intitle:{queries['intitle']}")

def _add_inurl(queries, query_parts):
    if queries.get('inurl'):
        query_parts.append(f"inurl:{queries['inurl']}")

def build_advanced_query(queries):
    """Build advanced search query with multiple query types."""
    query_parts = []
    _add_normal_query(queries, query_parts)
    _add_exact_phrase(queries, query_parts)
    _add_semantic_query(queries, query_parts)
    _add_include_terms(queries, query_parts)
    _add_exclude_terms(queries, query_parts)
    _add_filetype(queries, query_parts)
    _add_site_include(queries, query_parts)
    _add_site_exclude(queries, query_parts)
    _add_intitle(queries, query_parts)
    _add_inurl(queries, query_parts)
    return " ".join(query_parts)

def display_results(df, pages_retrieved, max_pages, final_query=None):
    """Display the search results and download buttons."""
    if df.empty:
        st.warning("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÉ‡∏î ‡πÜ")
        display_no_results_info()
    else:
        st.success(f"‡∏û‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå {len(df)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ ‡∏à‡∏≤‡∏Å {pages_retrieved} ‡∏´‡∏ô‡πâ‡∏≤")
        if final_query:
            st.info(f"**‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ:** `{final_query}`")
        if pages_retrieved < max_pages:
            st.info(f"‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: Scraping ‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡πÄ‡∏û‡∏µ‡∏¢‡∏á {pages_retrieved} ‡∏´‡∏ô‡πâ‡∏≤ ‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ {max_pages} ‡∏´‡∏ô‡πâ‡∏≤")
        
        st.dataframe(df, use_container_width=True)
        
        try:
            csv_data, excel_data = create_download_files(df)
            timestamp = int(time.time())
            
            col1, col2 = st.columns(2)
            with col1:
                st.download_button(
                    "‚¨áÔ∏è ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î CSV", 
                    data=csv_data, 
                    file_name=f"ddg_{timestamp}.csv",
                    mime="text/csv"
                )
            with col2:
                st.download_button(
                    "‚¨áÔ∏è ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Excel", 
                    data=excel_data, 
                    file_name=f"ddg_{timestamp}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )
        except Exception as e:
            st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î: {str(e)}")

def main():
    """Main Streamlit application."""
    st.set_page_config(
        page_title="ü¶Ü DuckDuckGo Advanced Scraper", 
        layout="wide",
        initial_sidebar_state="expanded"
    )
    st.title("ü¶Ü DuckDuckGo Advanced Scraper")
    st.markdown("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏Å‡∏£‡∏≠‡∏Å‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≤‡∏á ‡πÜ ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏Ñ‡∏•‡∏¥‡∏Å Search ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå")


    # Initialize session state
    if 'search_results' not in st.session_state:
        st.session_state.search_results = None
    if 'pages_retrieved' not in st.session_state:
        st.session_state.pages_retrieved = 0
    if 'last_query' not in st.session_state:
        st.session_state.last_query = ""
    if 'final_query_used' not in st.session_state:
        st.session_state.final_query_used = ""

    # Sidebar configuration
    st.sidebar.header("‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Scraper")
    max_pages = st.sidebar.number_input(
        "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏ô‡πâ‡∏≤", 
        min_value=1, 
        value=20, 
        step=1,
        help="‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏°‡∏≤‡∏Å‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏≠‡∏≤‡∏à‡∏ñ‡∏π‡∏Å‡∏ö‡∏•‡πá‡∏≠‡∏Å"
    )

    if max_pages > 100:
        st.sidebar.warning("‚ö†Ô∏è ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏°‡∏≤‡∏Å ‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î")

    # Main search inputs
    st.header("üîç ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤")
    
    col1, col2 = st.columns(2)
    
    with col1:
        normal_query = st.text_input(
            "‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ:",
            placeholder="‡πÄ‡∏ä‡πà‡∏ô artificial intelligence",
            help="‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏õ‡∏Å‡∏ï‡∏¥ - ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡πÉ‡∏î‡∏Ñ‡∏≥‡∏´‡∏ô‡∏∂‡πà‡∏á"
        )
        
        semantic_query = st.text_input(
            "‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö Semantic (~):",
            placeholder="‡πÄ‡∏ä‡πà‡∏ô machine learning",
            help="‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô"
        )
    
    with col2:
        exact_phrase = st.text_input(
            "‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ (\"):",
            placeholder="‡πÄ‡∏ä‡πà‡∏ô data science",
            help="‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ß‡∏•‡∏µ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô"
        )

    # Advanced search options in sidebar
    st.sidebar.header("üîß ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á")
    
    # Include/Exclude terms
    include_terms = st.sidebar.text_input(
        "‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ô‡πâ‡∏ô (+):",
        placeholder="python, tutorial",
        help="‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏Ñ‡∏±‡πà‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≠‡∏°‡∏°‡πà‡∏≤ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ô‡πâ‡∏ô‡πÉ‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå"
    )
    
    exclude_terms = st.sidebar.text_input(
        "‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô (-):",
        placeholder="basic, beginner",
        help="‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏Ñ‡∏±‡πà‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≠‡∏°‡∏°‡πà‡∏≤ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå"
    )
    
    # File type filter
    filetype = st.sidebar.selectbox(
        "‡∏ä‡∏ô‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå:",
        ["", "pdf", "doc", "docx", "xls", "xlsx", "ppt", "pptx", "html"],
        help="‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ä‡∏ô‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£"
    )
    
    # Site filters
    site_include = st.sidebar.text_input(
        "‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå‡πÄ‡∏â‡∏û‡∏≤‡∏∞ (site:):",
        placeholder="arxiv.org",
        help="‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÉ‡∏ô‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå‡∏ô‡∏µ‡πâ"
    )
    
    site_exclude = st.sidebar.text_input(
        "‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå (-site:):",
        placeholder="wikipedia.org",
        help="‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå‡∏ô‡∏µ‡πâ‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå"
    )
    
    # Title and URL filters
    intitle = st.sidebar.text_input(
        "‡πÉ‡∏ô Title (intitle:):",
        placeholder="research",
        help="‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡πÉ‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö"
    )
    
    inurl = st.sidebar.text_input(
        "‡πÉ‡∏ô URL (inurl:):",
        placeholder="blog",
        help="‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡πÉ‡∏ô URL ‡∏Ç‡∏≠‡∏á‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö"
    )

    # Build queries dict
    queries = {
        'normal_query': normal_query,
        'exact_phrase': exact_phrase,
        'semantic_query': semantic_query,
        'include_terms': include_terms,
        'exclude_terms': exclude_terms,
        'filetype': filetype,
        'site_include': site_include,
        'site_exclude': site_exclude,
        'intitle': intitle,
        'inurl': inurl
    }
    
    # Build final query
    final_query = build_advanced_query(queries)
    
    # Display the constructed query
    if final_query:
        st.info(f"**‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ:** `{final_query}`")
    
    # Validation
    has_any_query = any([normal_query.strip(), exact_phrase.strip(), semantic_query.strip()])
    
    # Clear previous results if query changed
    if final_query != st.session_state.last_query and final_query.strip():
        st.session_state.search_results = None
        st.session_state.pages_retrieved = 0
    
    if st.button("Search", type="primary"):
        if not has_any_query:
            st.error("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Å‡πà‡∏≠‡∏ô‡∏Ñ‡∏•‡∏¥‡∏Å Search")
            return

        scraper = DuckDuckGoScraper()
        
        with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤... ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏≠‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà"):
            try:
                df, pages_retrieved = scraper.scrape(final_query, max_pages, headless=True)
                st.session_state.search_results = df
                st.session_state.pages_retrieved = pages_retrieved
                st.session_state.last_query = final_query
                st.session_state.final_query_used = final_query
            except Exception as e:
                st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡πÅ‡∏Ñ‡∏£‡∏õ: {str(e)}")
                display_error_suggestions()
                return

    # Display results if they exist in session state
    if st.session_state.search_results is not None:
        df = st.session_state.search_results
        pages_retrieved = st.session_state.pages_retrieved
        final_query_used = st.session_state.final_query_used
        display_results(df, pages_retrieved, max_pages, final_query_used)


if __name__ == "__main__":
    main()